# llm_rag/rag_pipeline.py

from .retriever.retriever import retrieve_context
from .llm.llm_response import generate_response

def ask(question: str):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    (Retrieve -> Generate)
    """
    print(f"ğŸ”„ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: \"{question}\"")
    
    # 1. Retrieve: ê´€ë ¨ ë¬¸ì„œ ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    print("ğŸ” ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    context_str, sources = retrieve_context(question, k=5)
    
    if "Error:" in context_str:
        print(f"ì˜¤ë¥˜: {context_str}")
        return
    
    print("âœ… ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")
    
    # 2. Generate: LLMì— ë‹µë³€ ìš”ì²­
    print("ğŸ¤– ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    answer = generate_response(context_str, question)
    print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    
    return answer, context_str # ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì²˜ëŸ¼ ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜

# --- ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œ í…ŒìŠ¤íŠ¸ (ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì˜ main ë¶€ë¶„) ---
if __name__ == "__main__":
    test_question = "ê°•ì•„ì§€ í”¼ë¶€ê°€ ê±´ì¡°í•œë° ì˜¤ë©”ê°€3 ì˜ì–‘ì œ ì¶”ì²œí•´ì¤˜"
    
    final_answer, context = ask(test_question)
    
    print("\n" + "="*50)
    print(f"ì§ˆë¬¸: {test_question}")
    print(f"\në‹µë³€:\n{final_answer}")
    print(f"\n[ì°¸ê³ í•œ ìë£Œ]\n{context}")
    print("="*50)