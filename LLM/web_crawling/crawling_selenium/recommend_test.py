import pandas as pd
import os
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
# âœ… Gemini ëª¨ë¸ì„ ìœ„í•œ í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# --- 0. Google AI API í‚¤ ì„¤ì • ---
# í„°ë¯¸ë„ì—ì„œ Google AI API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
# export GOOGLE_API_KEY="AIza..."

if "GOOGLE_API_KEY" not in os.environ:
    print("âŒ ì˜¤ë¥˜: GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit()

# --- 1. ë°ì´í„° ë¡œë”© ë° ê°€ê³µ (ì´ì „ê³¼ ë™ì¼) ---
CSV_FILE_PATH = 'lifet_products_final.csv'
try:
    df = pd.read_csv(CSV_FILE_PATH)
    print(f"âœ… '{CSV_FILE_PATH}' íŒŒì¼ ë¡œë”© ì„±ê³µ! ({len(df)}ê°œ ìƒí’ˆ)")
except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: '{CSV_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

documents = []
for index, row in df.iterrows():
    content = f"ìƒí’ˆëª…ì€ '{row['NAME']}'ì´ë©°, ê°€ê²©ì€ {row['PRICE']}ì›ì…ë‹ˆë‹¤. í˜„ì¬ ë¦¬ë·° ìˆ˜ëŠ” {row['REVIEW_COUNT']}ê°œ, í‰ê·  í‰ì ì€ {row['RATING_AVG']}ì ì…ë‹ˆë‹¤."
    doc = Document(page_content=content, metadata={'product_code': row['PRODUCT_CODE']})
    documents.append(doc)
print(f"âœ… {len(documents)}ê°œì˜ ìƒí’ˆ ì •ë³´ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")


# --- 2. ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB(ChromaDB) ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---
print("\nì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
model_name = "jhgan/ko-sbert-nli" 
embeddings = HuggingFaceEmbeddings(model_name=model_name)
print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

db_directory = 'chroma_db'
vectorstore = Chroma.from_documents(
    documents, 
    embeddings, 
    persist_directory=db_directory
)
print(f"âœ… ìƒí’ˆ ì •ë³´ë¥¼ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# --- 3. Gemini LLM ë° RAG ì²´ì¸ ì„¤ì • ---
# âœ… Google Gemini ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.
# model="gemini-pro"ëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤.
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro",
                             temperature=0.1,
                             convert_system_message_to_human=True) # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì§€ì›í•˜ë„ë¡ ì„¤ì •

# RAG ì²´ì¸ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)
print("RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

# --- 4. ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ---
query = "ìš°ë¦¬ ê°•ì•„ì§€ê°€ 7ì‚´ì¸ë° ìŠ¬ê°œê³¨ì´ ì•ˆ ì¢‹ì•„. ê´€ì ˆì— ì¢‹ì€ ì˜ì–‘ì œ ì¶”ì²œí•´ ì¤˜."
print(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸: {query}")

response = qa_chain.invoke(query)

print("\nğŸ¤– AI ì¶”ì²œ ë‹µë³€:")
print(response['result'])